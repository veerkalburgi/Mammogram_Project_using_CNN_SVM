{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict whether a mammogram is bening or malignant\n",
    "   1. BI-RADS assessment: 1 to 5 (ordinal)  \n",
    "   2. Age: patient's age in years (integer)\n",
    "   3. Shape: mass shape: round=1 oval=2 lobular=3 irregular=4 (nominal)\n",
    "   4. Margin: mass margin: circumscribed=1 microlobulated=2 obscured=3 ill-defined=4 spiculated=5 (nominal)\n",
    "   5. Density: mass density high=1 iso=2 low=3 fat-containing=4 (ordinal)\n",
    "   6. Severity: benign=0 or malignant=1 (binominal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(\"mammographic_masses.data.txt\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So we go into the file and notice there is no heading and the missing values are denoted by '?'. We can take care of that by reading it again and specifying the name(using a list) and range of the data frames with parameters names and usecols respectively. We can also specify the missing values with na_values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"BI_RADS\", \"age\", \"shape\", \"margin\", \"density\", \"severity\"]\n",
    "\n",
    "df = pd.read_csv(\"mammographic_masses.data.txt\", names = header, usecols = range(6), na_values = '?')\n",
    "\n",
    "df.head()\n",
    "y=df.severity\n",
    "x=df.drop('severity',axis=1)\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\n",
    "x_train.head()\n",
    "x_train.shape\n",
    "x_test.shape\n",
    "print(x_train.shape[0],'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "#num_classes =2\n",
    "#convert class vectors to binary class matrices\n",
    "#y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "#x_train.shape\n",
    "#x_test.shape\n",
    "#y_train.shape\n",
    "#y_test.shape\n",
    "#print(x_train.shape[0],\"train samples\")\n",
    "#print(x_test.shape[0],\"test samples\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Did you notice the difference? So as we can see we are lucky all the datas are numerical. So lets analyze the datas using the .describe() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Everything looks good but there is a problem with BI_RADS it was given from 1 - 5 but we are having a maximum of 55 so what we going to do is look for all BI_RADS entry above 5. If the are not much we can easily drop them. Also we have some NaN values in age, shape and density, we will deal with it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['BI_RADS'][df['BI_RADS'] > 5]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Its noticed that where we have just 12 BI_RADS greater than 5 we have NaNs (double jeopardy) but for safety lets change it to NaN also. So we can easily drop it, but first lets look at where the rest NANs are and how many they are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BI_RADS'][df['BI_RADS'] > 5] = np.nan\n",
    "df.loc[(df['density'].isnull()) | \n",
    "       (df['age'].isnull()) |\n",
    "      (df['shape'].isnull()) |\n",
    "       (df['margin'].isnull()) |\n",
    "      df['BI_RADS'].isnull()]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For age we can solve the nan problem by replacing with the median since its value are continuous. Replacing with the median won't really affect the data since most of the entry is centered there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can easily calculate the median and find nan using numpy\n",
    "import numpy as np\n",
    "\n",
    "age_mean = np.mean(df['age'])\n",
    "age_nan = np.isnan(df['age'])\n",
    "df['age'][age_nan] = age_mean"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So we check either using .describe() or .info(). Lets use .info() this time around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For the rest NaN since they take a discrete and are randomly distributed around the dataframe we will drop them. So we use dropna to drop na and safe in same data frame using inplace = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so lets check and see if all is well\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So you see we have settled most of the problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can easily find the correlation of the datas with each other using heatmap. We will do that importing seaborn and matplotlib.pyplot.\n",
    "Enthusiastsss you can read about both in this links respectively.\n",
    "https://matplotlib.org/api/pyplot_api.html\n",
    "https://seaborn.pydata.org/generated/seaborn.heatmap.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f,ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(df.corr(), annot=True, linewidths=0.5, fmt= '.2f',ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As we can see severity has its highest correlation with itself (as expected), then also very high with margin and shape and lowest with density. With this we can see all the columns are needed to get the severity. So lets create our features (data we use to train our model -everything but severity-) and labels(what we want to predict-just severity-). We will also convert them to numpy arrays using .values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['severity'].values\n",
    "\n",
    "features = df[['BI_RADS', 'age', 'shape', 'margin', 'density']].values\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can either normalize our data or leave it like that. Decision trees do not require us to normalize our data but svm, neural networks etc do. So lets normalize using StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "scaled_features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So one important thing will be to split our data into training and testing data. This is done so we can easily fit and test our data. We split our data into 25% test size and 75% training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(scaled_features, labels, test_size= 0.25, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! as simple as it sounds it did not perform bad at all. So now lets try this last model. Quite interesting one\n",
    "# Neural Networks \n",
    "Here we are going to use Keras and sklearn which will in turn use a TensorFlow backend. You can get more information of Neural Networks and Keras here ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "#from keras.layers import Dense, Dropout\n",
    "#from keras.optimizers import RMSprop\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(x_input):\n",
    "    x = Dense(6,kernel_initializer='normal', activation='relu')(x_input)\n",
    "    x = Dense(6,kernel_initializer='normal', activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x_out = Dense(256, activation='relu')(x)\n",
    "    x_out = Dropout(0.5)(x)\n",
    "    return x_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(x_input):\n",
    "    x = Dense(4, activation='sigmoid')(x_input)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(4, activation='sigmoid')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x_out = Dense(12)(x)\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(5,))\n",
    "x      = model_1(inputs)\n",
    "x_out  = Dense(1, use_bias=False, activation='linear', name='svm')(x)\n",
    "model = Model(inputs, x_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss(layer):\n",
    "    weights = layer.weights[0]\n",
    "    weights_tf = tf.convert_to_tensor(weights)\n",
    "    \n",
    "    def categorical_hinge_loss(y_true, y_pred):\n",
    "        pos = K.sum(y_true * y_pred, axis=-1)\n",
    "        neg = K.max((1.0 - y_true) * y_pred, axis=-1)\n",
    "        hinge_loss = K.mean(K.maximum(0.0, neg - pos + 1), axis=-1)\n",
    "        regularization_loss = 0.5*(tf.reduce_sum(tf.square(weights_tf)))\n",
    "        return regularization_loss + 0.4*hinge_loss\n",
    "    \n",
    "    return categorical_hinge_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['accuracy']\n",
    "optimizer = tf.keras.optimizers.RMSprop(lr=2e-3, decay=1e-5)\n",
    "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
